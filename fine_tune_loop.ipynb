{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "jSI9Xza6rjtY",
        "-o91xpvpM96r",
        "w2P1ufAvPMAp",
        "P9Xk3fy8NcAg",
        "ni7xPYzYg4rK",
        "B9F8OcaxQgVA",
        "5iVBLBPDpOgW",
        "l4h4BaarGwKG",
        "vSZPuJS4tGa2",
        "LoSw39vwRdj1"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "497b7a3bba8a4176aad4dd64bea010cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9b30327e76e4ff6a0adcde7b498688a",
              "IPY_MODEL_da785ff916e842c0bd58ee0b5866b1be",
              "IPY_MODEL_b5aecc2fac4040b2afb6f5130002aecf"
            ],
            "layout": "IPY_MODEL_252d5fc69b524283b437a0c71f9adde1"
          }
        },
        "e9b30327e76e4ff6a0adcde7b498688a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f55928625a6c4a95a45f9d0142d359df",
            "placeholder": "​",
            "style": "IPY_MODEL_4e8ecf1e6574421da8122b748a1d5fd2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "da785ff916e842c0bd58ee0b5866b1be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_607c2a8cc8834d2483aabb608c2b0cc5",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd99ca079a1543b9a0df0ae456d338bd",
            "value": 8
          }
        },
        "b5aecc2fac4040b2afb6f5130002aecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea4849584958415a8747d82fc08c19c8",
            "placeholder": "​",
            "style": "IPY_MODEL_4e4c627fc207451b860f11644f38d372",
            "value": " 8/8 [01:14&lt;00:00,  8.18s/it]"
          }
        },
        "252d5fc69b524283b437a0c71f9adde1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f55928625a6c4a95a45f9d0142d359df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e8ecf1e6574421da8122b748a1d5fd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "607c2a8cc8834d2483aabb608c2b0cc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd99ca079a1543b9a0df0ae456d338bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea4849584958415a8747d82fc08c19c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e4c627fc207451b860f11644f38d372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "019f158a459a4049be9583de25ac854e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3402d1aa59d441d7898d5c252c90ed75",
              "IPY_MODEL_e626efb7d6f64d94a982af87a524121b",
              "IPY_MODEL_75db1e962bad45eabefd579b3f41c4a7"
            ],
            "layout": "IPY_MODEL_8a02b0e6c7f94a96b2254cfc72629cdf"
          }
        },
        "3402d1aa59d441d7898d5c252c90ed75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c11666983bc4d4f8c7923313efaf98a",
            "placeholder": "​",
            "style": "IPY_MODEL_13ea636a1f7649dc8cf769f0b07a297d",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "e626efb7d6f64d94a982af87a524121b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a22d22eebab74cfabfe574650c63bc80",
            "max": 237,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da21af8e24b54d0fadec7ec185c56a05",
            "value": 237
          }
        },
        "75db1e962bad45eabefd579b3f41c4a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89108a4651f44c848b3f6ed206325daf",
            "placeholder": "​",
            "style": "IPY_MODEL_2f3e0982470449c8b00bac4e9012dbac",
            "value": " 237/237 [00:00&lt;00:00, 5.57kB/s]"
          }
        },
        "8a02b0e6c7f94a96b2254cfc72629cdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c11666983bc4d4f8c7923313efaf98a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13ea636a1f7649dc8cf769f0b07a297d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a22d22eebab74cfabfe574650c63bc80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da21af8e24b54d0fadec7ec185c56a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89108a4651f44c848b3f6ed206325daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f3e0982470449c8b00bac4e9012dbac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ba2dbe953194eaea5aa15f11543e994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a8747253eb14189ad63868d9681dae7",
              "IPY_MODEL_0ad179dbed2a4af987aa174b6947bc1c",
              "IPY_MODEL_9b717a6f44774aa3bad68f0aee98b51c"
            ],
            "layout": "IPY_MODEL_b098963321a84a5892b005e40ce486d0"
          }
        },
        "0a8747253eb14189ad63868d9681dae7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_706014bc7a174f6083c0f5cfd9b47281",
            "placeholder": "​",
            "style": "IPY_MODEL_7237aef2be36409ebad08908ca51f5cf",
            "value": "Downloading (…)/main/tokenizer.json: "
          }
        },
        "0ad179dbed2a4af987aa174b6947bc1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a676245824e941aabcd0442bff521695",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd292ed253bd4e9492dd1a52e7415a99",
            "value": 1
          }
        },
        "9b717a6f44774aa3bad68f0aee98b51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_769e1125b54340369220d41d88df516b",
            "placeholder": "​",
            "style": "IPY_MODEL_80a1c8d92b8e40d3a868d1094af14af8",
            "value": " 2.11M/? [00:00&lt;00:00, 20.9MB/s]"
          }
        },
        "b098963321a84a5892b005e40ce486d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "706014bc7a174f6083c0f5cfd9b47281": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7237aef2be36409ebad08908ca51f5cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a676245824e941aabcd0442bff521695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fd292ed253bd4e9492dd1a52e7415a99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "769e1125b54340369220d41d88df516b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80a1c8d92b8e40d3a868d1094af14af8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39bcacc7755c478e80b7cc2e90401480": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cea24901a6dd45d996798b904a8e7ea4",
              "IPY_MODEL_46c066942c9c4743a1d9cfa328506219",
              "IPY_MODEL_00fafd62e7234fccbcd9adaed8ca583a"
            ],
            "layout": "IPY_MODEL_5f89f4ea70174ccf91dd7f93491994ae"
          }
        },
        "cea24901a6dd45d996798b904a8e7ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d52a1c2a97f94b9186beecca80dab92e",
            "placeholder": "​",
            "style": "IPY_MODEL_48651493f7984cbaaf9d62767a6c2c82",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "46c066942c9c4743a1d9cfa328506219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e09741008fb147138c762fa4acc4d63f",
            "max": 99,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a86ce191d3142adae9c78c4a2c1a7da",
            "value": 99
          }
        },
        "00fafd62e7234fccbcd9adaed8ca583a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e60a73cb067e4bc28f00133aeb70d258",
            "placeholder": "​",
            "style": "IPY_MODEL_cc4dc9dadee2493e8ca76ebb4453652d",
            "value": " 99.0/99.0 [00:00&lt;00:00, 1.43kB/s]"
          }
        },
        "5f89f4ea70174ccf91dd7f93491994ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d52a1c2a97f94b9186beecca80dab92e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48651493f7984cbaaf9d62767a6c2c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e09741008fb147138c762fa4acc4d63f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a86ce191d3142adae9c78c4a2c1a7da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e60a73cb067e4bc28f00133aeb70d258": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc4dc9dadee2493e8ca76ebb4453652d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08295b6e67fc4f2c8b9192a76537c407": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3275f24ed6a9481fb9713b42efbebb8a",
              "IPY_MODEL_cadbfada372f4bba9c94a4a2d2b0dcc3",
              "IPY_MODEL_f6e3708c521e4ed3b2bd18cae6f4773e"
            ],
            "layout": "IPY_MODEL_f7c0c6948a7d451a995fb7be715814e1"
          }
        },
        "3275f24ed6a9481fb9713b42efbebb8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e7f9491aeb0483eaeaaa6dbba87a628",
            "placeholder": "​",
            "style": "IPY_MODEL_32bf1b82aafe4380b5c0b4b6c6437e5e",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "cadbfada372f4bba9c94a4a2d2b0dcc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d1ccc15b9f04f6a8733c6b569250ec8",
            "max": 604,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbbfee1368d2406b97053695a00d7b7a",
            "value": 604
          }
        },
        "f6e3708c521e4ed3b2bd18cae6f4773e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dde26d09cd04314892edf6006f8fcd6",
            "placeholder": "​",
            "style": "IPY_MODEL_012088f8331a4094bd8f64d3b1ad8cd1",
            "value": " 604/604 [00:00&lt;00:00, 19.1kB/s]"
          }
        },
        "f7c0c6948a7d451a995fb7be715814e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7f9491aeb0483eaeaaa6dbba87a628": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32bf1b82aafe4380b5c0b4b6c6437e5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d1ccc15b9f04f6a8733c6b569250ec8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbbfee1368d2406b97053695a00d7b7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3dde26d09cd04314892edf6006f8fcd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "012088f8331a4094bd8f64d3b1ad8cd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "820535062924491b87ee623d5f115c9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2735827ae2fd490ba58f24435dc804f6",
              "IPY_MODEL_736c1f89100e44068abfa5e579ee350d",
              "IPY_MODEL_72537b9e415b408fb4c55376736e0ab2"
            ],
            "layout": "IPY_MODEL_38f497b6ddcb4cebb797ec1a83a8b894"
          }
        },
        "2735827ae2fd490ba58f24435dc804f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_53f96e01bbf64ea7881132a58376b3ec",
            "placeholder": "​",
            "style": "IPY_MODEL_1be12e2b5a1e4920bb72933e3d42715d",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "736c1f89100e44068abfa5e579ee350d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_750cfc919d25436c855cc454df798e8e",
            "max": 5686113497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aec960cbc2974db5b8b5b1a64b0b58ec",
            "value": 5686113497
          }
        },
        "72537b9e415b408fb4c55376736e0ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a46dc3d69e44720a2c487ba24cfd634",
            "placeholder": "​",
            "style": "IPY_MODEL_4de71367f8404193b6c9dcca8e0d54fb",
            "value": " 5.69G/5.69G [01:21&lt;00:00, 80.4MB/s]"
          }
        },
        "38f497b6ddcb4cebb797ec1a83a8b894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f96e01bbf64ea7881132a58376b3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be12e2b5a1e4920bb72933e3d42715d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "750cfc919d25436c855cc454df798e8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aec960cbc2974db5b8b5b1a64b0b58ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a46dc3d69e44720a2c487ba24cfd634": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4de71367f8404193b6c9dcca8e0d54fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "409d8950dcc94526abb59b84f2d2d538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84b63a4c80264a43937415caa96bd45b",
              "IPY_MODEL_82b181ab3a7643ad8f6e810490638954",
              "IPY_MODEL_a15dc40978504312be53290f7469c6ec"
            ],
            "layout": "IPY_MODEL_1d2081779a524b1f94ea4d06b6859620"
          }
        },
        "84b63a4c80264a43937415caa96bd45b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4860f6361bae4cc8828c68ab522c6cb6",
            "placeholder": "​",
            "style": "IPY_MODEL_ab7ea31527c64918a77974cecddf83b6",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "82b181ab3a7643ad8f6e810490638954": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67903277e7f74ae1a0b8ca8ef9fcf128",
            "max": 111,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a89bb3d99a3046678366b27625710f36",
            "value": 111
          }
        },
        "a15dc40978504312be53290f7469c6ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47b5de45de984b0a83f284b753bbd6d6",
            "placeholder": "​",
            "style": "IPY_MODEL_44c594b6d6ad4523ac419c7ca36c2c8a",
            "value": " 111/111 [00:00&lt;00:00, 2.56kB/s]"
          }
        },
        "1d2081779a524b1f94ea4d06b6859620": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4860f6361bae4cc8828c68ab522c6cb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab7ea31527c64918a77974cecddf83b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67903277e7f74ae1a0b8ca8ef9fcf128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a89bb3d99a3046678366b27625710f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47b5de45de984b0a83f284b753bbd6d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44c594b6d6ad4523ac419c7ca36c2c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BoxOfCereal/Fine-Tuning-Loop/blob/main/fine_tune_loop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FTL - Fine Tune Loop"
      ],
      "metadata": {
        "id": "RXN1NXKnGpd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intro"
      ],
      "metadata": {
        "id": "_GT-3oclR3Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook I will attempt to show all the steps necessary to go from data to text generation. The main headings will demonstrate the easiest way to go through a whole training Loop including loading data, loading your model from the hugging face ecosystem, training the model, benchmarking the model, inferencing the model, and taking that model and using it in your prompt library in our case we'll be using line chain"
      ],
      "metadata": {
        "id": "nFdaFEcHSstZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The subsections of each heading will contain more in depth variations of each of these steps. It is my hope that seeing multiple examples that are trying to accomplish the same thing will show the underlying patterns needed to not only understand how to collect data train a model and run inference on it but also adapt it to your use case with your own custom data, model, and inferencing needs."
      ],
      "metadata": {
        "id": "f8gL6-8lSzIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is designed to take you from data to application with large language models. A Star Emoji is marked to show which path is recommended for a first time use"
      ],
      "metadata": {
        "id": "VojPXyE6PrGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was adapted from hugging faces blog post about training falcon using QLoRA. Which can be found [here](https://huggingface.co/blog/falcon) and [here is the associated notebook](https://colab.research.google.com/drive/1BiQiw31DT7-cDp1-0ySXvvhzqomTdI-o?usp=sharing).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "A2DLUpqakAOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-requisites:\n",
        "* A hugging face account - [Sign up](https://huggingface.co/join)\n",
        "* A weights and biases account [Sign Up](https://wandb.ai/login?signup=true)\n",
        "* Some python experience\n",
        "* Some basic experience with large language models\n",
        "[Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)"
      ],
      "metadata": {
        "id": "mSViaMX56hMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install --upgrade huggingface_hub\n",
        "from huggingface_hub import login, whoami, create_repo\n",
        "login(token=\"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\")\n",
        "\n",
        "user = whoami()['name']\n",
        "# repo_id = f'{user}/hf-hub-modelcards-pr-test'\n"
      ],
      "metadata": {
        "id": "tCQNtZpMfulP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Legend:\n",
        "⭐ - Recommended"
      ],
      "metadata": {
        "id": "BykO_Wa5PTbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning a model"
      ],
      "metadata": {
        "id": "3W2vwdr0VLYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to this Google Colab notebook that demonstrates how to fine-tune a language model using QLoRA on a single Google Colab instance. We will leverage the PEFT library from the Hugging Face ecosystem for efficient fine-tuning. With QLoRA, we can make the most of limited memory resources during the fine-tuning process. Let's get started and transform this model into a chatbot that you can interact with!"
      ],
      "metadata": {
        "id": "C2EgqEPDQ8v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️⃣ Dataset"
      ],
      "metadata": {
        "id": "Rnqmq7amRrU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "gN80l0IXeGVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### \"timdettmers/openassistant-guanaco\" ⭐"
      ],
      "metadata": {
        "id": "7qJ2XH7jPBhv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our experiment, we will use the Guanaco dataset, which is a clean subset of the OpenAssistant dataset adapted to train general purpose chatbots.\n",
        "\n",
        "The dataset can be found [here](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)."
      ],
      "metadata": {
        "id": "PkpVutO3ubL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ],
      "metadata": {
        "id": "0X3kHnskSWU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at what's inside the data set. It's always important to understand what you're feeding your model."
      ],
      "metadata": {
        "id": "XG2UUQOABxUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "KxFDjTVcBUBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want some more information you can check out [the hugging face documentation](https://huggingface.co/docs/datasets/access)"
      ],
      "metadata": {
        "id": "GgDXDskPvdoD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Custom Dataset (WIP!)"
      ],
      "metadata": {
        "id": "jSI9Xza6rjtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[LIMA: Less Is More for Alignment](https://arxiv.org/pdf/2305.11206.pdf)\n",
        "\n",
        "We observe that, for the purpose of alignment, scaling up input diversity and output quality have\n",
        "measurable positive effects, while scaling up quantity alone might not.\n",
        "\n",
        "how much data is needed to teach a pre-trained large language model new factual information?"
      ],
      "metadata": {
        "id": "mDkDXuAEEoG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Textbooks Are All You Need](https://arxiv.org/abs/2306.11644)\n",
        "[autolabel](https://github.com/refuel-ai/autolabel)"
      ],
      "metadata": {
        "id": "_hMpXq7SMc3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A filtered code-language dataset, which is a subset\n",
        "of The Stack and StackOverflow, obtained by\n",
        "using a language model-based classifier (consisting of about 6B tokens).\n",
        "* A synthetic textbook dataset consisting of <1B tokens of GPT-3.5 generated Python textbooks.\n",
        "* A small synthetic exercises dataset consisting of ∼180M tokens of Python exercises and solutions.\n",
        "\n",
        "Filtering of existing code datasets using a transformer-based classifie"
      ],
      "metadata": {
        "id": "WnUJ2ffD8iRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " The following example demonstrates the synthetically generated textbook text:\n",
        "\n",
        "To begin, let us define singular and nonsingular matrices. A matrix is said to be singular if its\n",
        "determinant is zero. On the other hand, a matrix is said to be nonsingular if its determinant is not\n",
        "zero. Now, let's explore these concepts through examples.\n",
        "Example 1:\n",
        "Consider the matrix A = np.array([[1, 2], [2, 4]]). We can check if this matrix is singular or\n",
        "nonsingular using the determinant function. We can define a Python function, `is_singular(A)`, which\n",
        "returns true if the determinant of A is zero, and false otherwise.\n",
        "```python\n",
        "import numpy as np\n",
        "def is_singular(A):\n",
        "det = np.linalg.det(A)\n",
        "if det == 0:\n",
        "return True\n",
        "else:\n",
        "return False\n",
        "A = np.array([[1, 2], [2, 4]])\n",
        "print(is_singular(A)) # True\n",
        "```"
      ],
      "metadata": {
        "id": "tKYztyN08-p2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDbCu_Q09LiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2️⃣ Loading the model"
      ],
      "metadata": {
        "id": "rjOMoSbGSxx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🦙 togethercomputer/RedPajama-INCITE-Base-3B-v1 ⭐"
      ],
      "metadata": {
        "id": "-o91xpvpM96r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes)."
      ],
      "metadata": {
        "id": "YZpbG_g3ewBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q -U git+https://github.com/lvwerra/trl.git git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/accelerate.git git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes wandb"
      ],
      "metadata": {
        "id": "OeKOxWAteR93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
        "repo_name = f\"{user}/{model_name.split('/')[-1]}-SFT-guanaco-lora\""
      ],
      "metadata": {
        "id": "EOyksJG4R4fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "MIN_TRANSFORMERS_VERSION = '4.25.1'\n",
        "\n",
        "# check transformers version\n",
        "assert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n",
        "\n",
        "# init\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# model = model.to('cuda:0')\n",
        "\n"
      ],
      "metadata": {
        "id": "u0yGLhoFNE_Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761,
          "referenced_widgets": [
            "019f158a459a4049be9583de25ac854e",
            "3402d1aa59d441d7898d5c252c90ed75",
            "e626efb7d6f64d94a982af87a524121b",
            "75db1e962bad45eabefd579b3f41c4a7",
            "8a02b0e6c7f94a96b2254cfc72629cdf",
            "3c11666983bc4d4f8c7923313efaf98a",
            "13ea636a1f7649dc8cf769f0b07a297d",
            "a22d22eebab74cfabfe574650c63bc80",
            "da21af8e24b54d0fadec7ec185c56a05",
            "89108a4651f44c848b3f6ed206325daf",
            "2f3e0982470449c8b00bac4e9012dbac",
            "3ba2dbe953194eaea5aa15f11543e994",
            "0a8747253eb14189ad63868d9681dae7",
            "0ad179dbed2a4af987aa174b6947bc1c",
            "9b717a6f44774aa3bad68f0aee98b51c",
            "b098963321a84a5892b005e40ce486d0",
            "706014bc7a174f6083c0f5cfd9b47281",
            "7237aef2be36409ebad08908ca51f5cf",
            "a676245824e941aabcd0442bff521695",
            "fd292ed253bd4e9492dd1a52e7415a99",
            "769e1125b54340369220d41d88df516b",
            "80a1c8d92b8e40d3a868d1094af14af8",
            "39bcacc7755c478e80b7cc2e90401480",
            "cea24901a6dd45d996798b904a8e7ea4",
            "46c066942c9c4743a1d9cfa328506219",
            "00fafd62e7234fccbcd9adaed8ca583a",
            "5f89f4ea70174ccf91dd7f93491994ae",
            "d52a1c2a97f94b9186beecca80dab92e",
            "48651493f7984cbaaf9d62767a6c2c82",
            "e09741008fb147138c762fa4acc4d63f",
            "1a86ce191d3142adae9c78c4a2c1a7da",
            "e60a73cb067e4bc28f00133aeb70d258",
            "cc4dc9dadee2493e8ca76ebb4453652d",
            "08295b6e67fc4f2c8b9192a76537c407",
            "3275f24ed6a9481fb9713b42efbebb8a",
            "cadbfada372f4bba9c94a4a2d2b0dcc3",
            "f6e3708c521e4ed3b2bd18cae6f4773e",
            "f7c0c6948a7d451a995fb7be715814e1",
            "6e7f9491aeb0483eaeaaa6dbba87a628",
            "32bf1b82aafe4380b5c0b4b6c6437e5e",
            "6d1ccc15b9f04f6a8733c6b569250ec8",
            "fbbfee1368d2406b97053695a00d7b7a",
            "3dde26d09cd04314892edf6006f8fcd6",
            "012088f8331a4094bd8f64d3b1ad8cd1",
            "820535062924491b87ee623d5f115c9c",
            "2735827ae2fd490ba58f24435dc804f6",
            "736c1f89100e44068abfa5e579ee350d",
            "72537b9e415b408fb4c55376736e0ab2",
            "38f497b6ddcb4cebb797ec1a83a8b894",
            "53f96e01bbf64ea7881132a58376b3ec",
            "1be12e2b5a1e4920bb72933e3d42715d",
            "750cfc919d25436c855cc454df798e8e",
            "aec960cbc2974db5b8b5b1a64b0b58ec",
            "6a46dc3d69e44720a2c487ba24cfd634",
            "4de71367f8404193b6c9dcca8e0d54fb",
            "409d8950dcc94526abb59b84f2d2d538",
            "84b63a4c80264a43937415caa96bd45b",
            "82b181ab3a7643ad8f6e810490638954",
            "a15dc40978504312be53290f7469c6ec",
            "1d2081779a524b1f94ea4d06b6859620",
            "4860f6361bae4cc8828c68ab522c6cb6",
            "ab7ea31527c64918a77974cecddf83b6",
            "67903277e7f74ae1a0b8ca8ef9fcf128",
            "a89bb3d99a3046678366b27625710f36",
            "47b5de45de984b0a83f284b753bbd6d6",
            "44c594b6d6ad4523ac419c7ca36c2c8a"
          ]
        },
        "outputId": "b45f3489-6c2d-42ff-8e43-d96761626eac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "019f158a459a4049be9583de25ac854e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3ba2dbe953194eaea5aa15f11543e994"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39bcacc7755c478e80b7cc2e90401480"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/604 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08295b6e67fc4f2c8b9192a76537c407"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/5.69G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "820535062924491b87ee623d5f115c9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('8013'), PosixPath('http'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-1puys5tdjwn39 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "409d8950dcc94526abb59b84f2d2d538"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.modules"
      ],
      "metadata": {
        "id": "JLahXmmwWW_F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f9297d-8e5f-4b00-8494-87995a00cd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of GPTNeoXForCausalLM(\n",
              "  (gpt_neox): GPTNeoXModel(\n",
              "    (embed_in): Embedding(50432, 2560)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x GPTNeoXLayer(\n",
              "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (post_attention_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention): GPTNeoXAttention(\n",
              "          (rotary_emb): RotaryEmbedding()\n",
              "          (query_key_value): Linear4bit(in_features=2560, out_features=7680, bias=True)\n",
              "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
              "        )\n",
              "        (mlp): GPTNeoXMLP(\n",
              "          (dense_h_to_4h): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
              "          (dense_4h_to_h): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
              "          (act): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (embed_out): Linear(in_features=2560, out_features=50432, bias=False)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "FBTIWfc8Wk9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🦅 ybelkada/falcon-7b-sharded-bf16 in 4bit"
      ],
      "metadata": {
        "id": "w2P1ufAvPMAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes). We will also install `einops` as it is a requirement to load Falcon models."
      ],
      "metadata": {
        "id": "2lsKH80ueeqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q -U git+https://github.com/lvwerra/trl.git git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/accelerate.git git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb #einops for falcon"
      ],
      "metadata": {
        "id": "X4KXfk1eePky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will load the [Falcon 7B model](https://huggingface.co/tiiuae/falcon-7b), quantize it in 4bit and attach LoRA adapters on it. Let's get started!"
      ],
      "metadata": {
        "id": "AjB0WAqFSzlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
        "\n",
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "ZwXZbQ2dSwzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also load the tokenizer below"
      ],
      "metadata": {
        "id": "xNqIYtQcUBSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "XDS2yYmlUAD6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "84464ecf-cbad-45dc-df47-064b3ce6e88e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 1>\u001b[0m:\u001b[94m1\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'model_name'\u001b[0m is not defined\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 1&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'model_name'</span> is not defined\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we will load the configuration file in order to create the LoRA model. According to [QLoRA paper](https://arxiv.org/abs/2305.14314), it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `dense`, `dense_h_to_4_h` and `dense_4h_to_h` layers in the target modules in addition to the mixed query key value layer.\n",
        "\n",
        "A really good video on QLoRA is[AemonAlgiz](https://www.youtube.com/@AemonAlgiz)'s video\n",
        "[QLoRA Is More Than Memory Optimization. Train Your Models With 10% of the Data for More Performance.](https://youtu.be/v6sf4EF45fI) . WARNING: he does go into some math, but even if you don't understand it all ( which I certainly don't ) he explains it in a very satisfying way."
      ],
      "metadata": {
        "id": "NuAx3zBeUL1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "dQdvjTYTT1vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the models modules so we can see what we're targeting and how to find the linear modules in any other architecture we're interested in:"
      ],
      "metadata": {
        "id": "kIMf5UGj-kes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.modules"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wla1tvRx8hcD",
        "outputId": "413b406d-c691-4dae-be8a-29181cbda52b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method Module.modules of RWForCausalLM(\n",
              "  (transformer): RWModel(\n",
              "    (word_embeddings): Embedding(65024, 4544)\n",
              "    (h): ModuleList(\n",
              "      (0-31): 32 x DecoderLayer(\n",
              "        (input_layernorm): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
              "        (self_attention): Attention(\n",
              "          (maybe_rotary): RotaryEmbedding()\n",
              "          (query_key_value): Linear4bit(in_features=4544, out_features=4672, bias=False)\n",
              "          (dense): Linear4bit(in_features=4544, out_features=4544, bias=False)\n",
              "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (mlp): MLP(\n",
              "          (dense_h_to_4h): Linear4bit(in_features=4544, out_features=18176, bias=False)\n",
              "          (act): GELU(approximate='none')\n",
              "          (dense_4h_to_h): Linear4bit(in_features=18176, out_features=4544, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((4544,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4544, out_features=65024, bias=False)\n",
              ")>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###🦙🦅 Inference ⭐"
      ],
      "metadata": {
        "id": "P9Xk3fy8NcAg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No matter which model you load above This inference should work with either although expect different responses"
      ],
      "metadata": {
        "id": "4SEg1Vd9OFZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# infer\n",
        "prompt = \"Alan Turing is\"\n",
        "inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
        "input_length = inputs.input_ids.shape[1]\n",
        "outputs = model.generate(\n",
        "    **inputs, max_new_tokens=128, do_sample=True, temperature=0.7, top_p=0.7, top_k=50, return_dict_in_generate=True,\n",
        ")\n",
        "token = outputs.sequences[0, input_length:]\n",
        "output_str = tokenizer.decode(token)\n",
        "print(output_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "YXwkcmjSNfoN",
        "outputId": "e7eae6da-471c-445f-dc94-e0df017a3b0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " one of the most famous British scientists and mathematicians. He is famous for his work on the design of the first computer, the Turing machine.\n",
            "He was also a gay man who was persecuted for his sexuality during the time of his life. He was arrested and forced to take hormone therapy.\n",
            "He died by suicide in 1954.\n",
            "Turing’s work was not only important in the field of mathematics, but also in the field of computing.\n",
            "He is considered to be one of the greatest mathematicians of all time.\n",
            "Turing’s work was so influential that it is still used today.\n",
            "He was\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\na name that has been synonymous with the computer age since the 1950s. The British mathematician, logician, and cryptanalyst is widely regarded as the father of modern computing. His contributions to the development of the modern computer and the theory of computation have had a profound impact on the world we live in today.\\nTuring’s contributions to the development of the modern computer were made in the 1940s and 1950s. He is most famous for his work on the Turing machine, a theoretical model of a computing machine that was able to perform all the mathematical operations of a computer. Turing’s work on the...\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3️⃣ Loading the trainer"
      ],
      "metadata": {
        "id": "dzsYHLwIZoLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🦅🦙 Supervised Fine-tuning Trainer QLoRA 4-Bit ⭐"
      ],
      "metadata": {
        "id": "poPQAdX7PcWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below."
      ],
      "metadata": {
        "id": "aTBJVE4PaJwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "output_dir = \"./results\"\n",
        "per_device_train_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "optim = \"paged_adamw_32bit\"\n",
        "save_steps = 10\n",
        "logging_steps = 10\n",
        "learning_rate = 2e-4\n",
        "max_grad_norm = 0.3\n",
        "max_steps = 500\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        ")"
      ],
      "metadata": {
        "id": "OCFTvGW6aspE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are all these training arguments?\n",
        "\n",
        " Depending on your experience some may seem obvious and others not so much. This [documentation on training on one GPU](https://huggingface.co/docs/transformers/perf_train_gpu_one) from the hugging face Docs will go over many of the arguments that are dedicated to Performance."
      ],
      "metadata": {
        "id": "LnylR2N9vu9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then finally pass everthing to the trainer"
      ],
      "metadata": {
        "id": "I3t6b2TkcJwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "max_seq_length = 1024\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "TNeOBgZeTl2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also pre-process the model by upcasting the layer norms in float 32 for more stable training"
      ],
      "metadata": {
        "id": "GWplqqDjb3sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in trainer.model.named_modules():\n",
        "    if \"norm\" in name:\n",
        "        module = module.to(torch.float32)"
      ],
      "metadata": {
        "id": "7OyIvEx7b1GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train the model"
      ],
      "metadata": {
        "id": "1JApkSrCcL3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's train the model! Simply call `trainer.train()`\n",
        "\n",
        "TRL by default sends statistics to the weights and biases website which is why you need an account. You can also use tensor board however, let's be lazy and use WNB."
      ],
      "metadata": {
        "id": "JjvisllacNZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "_kbS7nRxcMt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking a look at the state of the trainer isn't necessary but it gives you some information about what the trainer actually tracks. If you're interested in that kind of thing."
      ],
      "metadata": {
        "id": "i1NOI77cr64R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.state"
      ],
      "metadata": {
        "id": "IU-UvNdds5p1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's save the model. TRL is handy because it automatically takes care of only saving our adapter. Which is much smaller than the original model."
      ],
      "metadata": {
        "id": "MBFQUqNasH9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "0vB-Mlt9tTGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4️⃣Inference"
      ],
      "metadata": {
        "id": "lZ8rNJODPubG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we finished training our adapter let's try out how it works! We will use the hugging face pipeline for text generation."
      ],
      "metadata": {
        "id": "bbQOw-L1wvSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🦙🦅 pipeline ⭐"
      ],
      "metadata": {
        "id": "IF2aM_4BmZ6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "p = \"\"\"### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant:\"\"\"\n",
        "sequences = pipeline(\n",
        "    p,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "id": "M8FfBNtGmecg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5️⃣ Saving to Hub"
      ],
      "metadata": {
        "id": "M6arq3_uLbi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section will save the trained adapter to the hugging face hub and inside of repo denoted by `repo_name`."
      ],
      "metadata": {
        "id": "7iNYhKWBrC5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import whoami #create_repo\n",
        "\n",
        "user = whoami()['name']\n",
        "model_name = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
        "repo_name = f\"{user}/{model_name.split('/')[-1]}-SFT-guanaco-lora\"\n",
        "print(repo_name)"
      ],
      "metadata": {
        "id": "JumMnNhQhvpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc0652a-ef62-4f40-8a11-fd5ee4f1cb2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nolestock/RedPajama-INCITE-Base-3B-v1-SFT-guanaco-lora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🦙🦅 Making a model card ⭐"
      ],
      "metadata": {
        "id": "odglYq5QsmLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model cards are not necessary to upload a model to the hugging face hub. However, they allow other people to get an idea of your model and how it might be useful to them. We're going to use a default template but the [hugging face model card documentation](https://huggingface.co/docs/huggingface_hub/guides/model-cards) will have more information."
      ],
      "metadata": {
        "id": "Pp3kyDrxifn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install Jinja2"
      ],
      "metadata": {
        "id": "HVOWAloLzHUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import ModelCard, ModelCardData\n",
        "card_data = ModelCardData(language='en', license='mit', library_name='keras')\n",
        "card = ModelCard.from_template(\n",
        "    card_data,\n",
        "    model_id=f'{model_name.split('/')[-1]}-SFT-guanaco-lora\"',\n",
        "    model_description=\"this model does this and that\",\n",
        "    developers=\"Nate Raw\",\n",
        "    repo=repo_name,\n",
        ")\n",
        "card.save('my_model_card_2.md')\n",
        "print(card)"
      ],
      "metadata": {
        "id": "C7JzojzpzIaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 🦙🦅 Push to the hub ⭐"
      ],
      "metadata": {
        "id": "1XSOISa2tbJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's push our model card and the model to our hugging face repo."
      ],
      "metadata": {
        "id": "B_wzJCc0tvMB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.push_to_hub(repo_name)\n",
        "card.push_to_hub(repo_name)"
      ],
      "metadata": {
        "id": "s-O4xfoQVVqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That was easy huh?"
      ],
      "metadata": {
        "id": "L7kTaK0Xt1az"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading (RESTART)"
      ],
      "metadata": {
        "id": "ni7xPYzYg4rK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will be loading the model after a fresh restart so we can see how to load models and adapters and create a model for inference. We will also be showing how to specify just the adapter and load a model for inference."
      ],
      "metadata": {
        "id": "oXuAeGKmpOt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🦙 togethercomputer/RedPajama-INCITE-Base-3B-v1 ⭐"
      ],
      "metadata": {
        "id": "126FMJ53m-VJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regardless of which way we decide to load the model we need to install the required packages."
      ],
      "metadata": {
        "id": "N17ygkRRp8Z1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q -U git+https://github.com/lvwerra/trl.git git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/accelerate.git git+https://github.com/huggingface/peft.git\n",
        "!pip install -q bitsandbytes wandb\n",
        "!pip install --upgrade huggingface_hub"
      ],
      "metadata": {
        "id": "s60IMVrrm-VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### From Huggingface hub (Model and Adapter) ⭐"
      ],
      "metadata": {
        "id": "4zL40Vl1QJ7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, whoami, create_repo\n",
        "login(token=\"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\")\n",
        "\n",
        "user = whoami()['name']\n",
        "model_name = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
        "repo_name = f\"{user}/{model_name.split('/')[-1]}-SFT-guanaco-lora\" #adapter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqMqfIEzm-VN",
        "outputId": "d5f2008b-9a98-403c-e8a6-ea64a410dd6a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "MIN_TRANSFORMERS_VERSION = '4.25.1'\n",
        "\n",
        "# check transformers version\n",
        "assert transformers.__version__ >= MIN_TRANSFORMERS_VERSION, f'Please upgrade transformers to version {MIN_TRANSFORMERS_VERSION} or higher.'\n",
        "\n",
        "# init\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(model, repo_name)\n",
        "\n",
        "# model = model.to('cuda:0')\n",
        "\n"
      ],
      "metadata": {
        "id": "DbrKxgB9m-VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### (Adapter only) ⭐"
      ],
      "metadata": {
        "id": "ckwvG_IXulwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login, whoami, create_repo\n",
        "login(token=\"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\")\n",
        "\n",
        "user = whoami()['name']\n",
        "model_name = \"togethercomputer/RedPajama-INCITE-Base-3B-v1\"\n",
        "repo_name = f\"{user}/{model_name.split('/')[-1]}-SFT-guanaco-lora\" #adapter"
      ],
      "metadata": {
        "id": "L-gaOt10u7CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n",
        "\n",
        "\n",
        "config = PeftConfig.from_pretrained(repo_name)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             return_dict=True,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             trust_remote_code=True,\n",
        "                                             device_map={\"\":0})"
      ],
      "metadata": {
        "id": "v5BBM0N3utDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "MFeuCUinQa-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### pipeline"
      ],
      "metadata": {
        "id": "B9F8OcaxQgVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "p = \"\"\"### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant:\"\"\"\n",
        "sequences = pipeline(\n",
        "    p,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n"
      ],
      "metadata": {
        "id": "8-okFrTXQJ80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### more control\n"
      ],
      "metadata": {
        "id": "5iVBLBPDpOgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See [behind the pipeline](https://huggingface.co/learn/nlp-course/chapter2/2?fw=pt) page for more info."
      ],
      "metadata": {
        "id": "0DhDh-iCv0EM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer, GenerationConfig\n",
        "\n",
        "peft_model_id = \"Bruno/Harpia-7b-guanacoLora\"\n",
        "\n",
        "config = PeftConfig.from_pretrained(peft_model_id)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,\n",
        "                                             return_dict=True,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             trust_remote_code=True,\n",
        "                                             device_map={\"\":0})\n",
        "\n",
        "\n",
        "prompt_input = \"\"\n",
        "prompt_no_input = \"\"\n",
        "\n",
        "def create_prompt(instruction, input=None):\n",
        "  if input:\n",
        "    return  prompt_input.format(instruction=instruction, input=input)\n",
        "  else:\n",
        "    return prompt_no_input.format(instruction=instruction)\n",
        "\n",
        "def generate(\n",
        "        instruction,\n",
        "        input=None,\n",
        "        max_new_tokens=128,\n",
        "        temperature=0.1,\n",
        "        top_p=0.75,\n",
        "        top_k=40,\n",
        "        num_beams=4,\n",
        "        **kwargs,\n",
        "):\n",
        "    prompt = create_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
        "    attention_mask = inputs[\"attention_mask\"].to(\"cuda\")\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        **kwargs,\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    return output.split(\"### Respuesta:\")[1]\n",
        "\n",
        "instruction = \"Me conte algumas curiosidades sobre o Brasil\"\n",
        "\n",
        "print(\"Instruções:\", instruction)\n",
        "print(\"Resposta:\", generate(instruction))\n"
      ],
      "metadata": {
        "id": "BNHeIi27pOHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging (OPTIONAL)(RESTART)\n",
        "[link text](https://github.com/lm-sys/FastChat/blob/main/fastchat/model/apply_lora.py)"
      ],
      "metadata": {
        "id": "l4h4BaarGwKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model already loaded"
      ],
      "metadata": {
        "id": "kHTb6HaowofA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q -U git+https://github.com/lvwerra/trl.git git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/accelerate.git git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops wandb #einops for falcon"
      ],
      "metadata": {
        "id": "Fyl621bbzGMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "repo_name = f\"nolestock/{model_name.split('/')[-1]}-finetuned-guanaco-lora\""
      ],
      "metadata": {
        "id": "XdzA5SNUy1Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftConfig, PeftModel\n",
        "from peft import LoraConfig\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    # load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, repo_name)\n",
        "\n",
        "print(\"Applying the LoRA\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(f\"Saving the target model\")\n",
        "# ValueError: Cannot merge LORA layers when the model is loaded in 8-bit mode\n",
        "model.save_pretrained()\n",
        "tokenizer.save_pretrained()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "497b7a3bba8a4176aad4dd64bea010cb",
            "e9b30327e76e4ff6a0adcde7b498688a",
            "da785ff916e842c0bd58ee0b5866b1be",
            "b5aecc2fac4040b2afb6f5130002aecf",
            "252d5fc69b524283b437a0c71f9adde1",
            "f55928625a6c4a95a45f9d0142d359df",
            "4e8ecf1e6574421da8122b748a1d5fd2",
            "607c2a8cc8834d2483aabb608c2b0cc5",
            "fd99ca079a1543b9a0df0ae456d338bd",
            "ea4849584958415a8747d82fc08c19c8",
            "4e4c627fc207451b860f11644f38d372"
          ]
        },
        "id": "sWijWn16w3qg",
        "outputId": "f4e3fa99-cfba-4cb0-f93d-fad2abb46143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('8013'), PosixPath('//172.28.0.1')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-1rmao5cihc2aw --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "497b7a3bba8a4176aad4dd64bea010cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 16>\u001b[0m:\u001b[94m16\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/peft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m202\u001b[0m in \u001b[92mfrom_pretrained\u001b[0m                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 199 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel = \u001b[96mcls\u001b[0m(model, config, adapter_name)                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 200 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 201 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config, ad  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 202 \u001b[2m│   │   \u001b[0mmodel.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 203 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m model                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 204 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 205 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_setup_prompt_encoder\u001b[0m(\u001b[96mself\u001b[0m, adapter_name):                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/peft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m464\u001b[0m in \u001b[92mload_adapter\u001b[0m                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 461 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m use_safetensors:                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 462 \u001b[0m\u001b[2m│   │   │   \u001b[0madapters_weights = safe_load_file(filename, device=\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m \u001b[94mif\u001b[0m torch.cuda.is_a  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 463 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 464 \u001b[2m│   │   │   \u001b[0madapters_weights = torch.load(                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 465 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mfilename, map_location=torch.device(\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m \u001b[94mif\u001b[0m torch.cuda.is_available()   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 466 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 467 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m809\u001b[0m in \u001b[92mload\u001b[0m                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 806 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, _weights_only_unpickl  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 807 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mRuntimeError\u001b[0m \u001b[94mas\u001b[0m e:                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 808 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[94mraise\u001b[0m pickle.UnpicklingError(UNSAFE_MESSAGE + \u001b[96mstr\u001b[0m(e)) \u001b[94mfrom\u001b[0m \u001b[96mNone\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 809 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _load(opened_zipfile, map_location, pickle_module, **pickle_load_  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 810 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m weights_only:                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 811 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 812 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m _legacy_load(opened_file, map_location, _weights_only_unpickler,   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m1172\u001b[0m in \u001b[92m_load\u001b[0m                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1169 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1170 \u001b[0m\u001b[2m│   \u001b[0munpickler = UnpicklerWrapper(data_file, **pickle_load_args)                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1171 \u001b[0m\u001b[2m│   \u001b[0munpickler.persistent_load = persistent_load                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1172 \u001b[2m│   \u001b[0mresult = unpickler.load()                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1173 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1174 \u001b[0m\u001b[2m│   \u001b[0mtorch._utils._validate_loaded_sparse_tensors()                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1175 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m1142\u001b[0m in \u001b[92mpersistent_load\u001b[0m           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1139 \u001b[0m\u001b[2m│   │   │   \u001b[0mtyped_storage = loaded_storages[key]                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   │   \u001b[0mnbytes = numel * torch._utils._element_size(dtype)                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1142 \u001b[2m│   │   │   \u001b[0mtyped_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1143 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1144 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m typed_storage                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1145 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m1116\u001b[0m in \u001b[92mload_tensor\u001b[0m               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1113 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# TODO: Once we decide to break serialization FC, we can\u001b[0m                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# stop wrapping with TypedStorage\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1115 \u001b[0m\u001b[2m│   │   \u001b[0mtyped_storage = torch.storage.TypedStorage(                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1116 \u001b[2m│   │   │   \u001b[0mwrap_storage=restore_location(storage, location),                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1117 \u001b[0m\u001b[2m│   │   │   \u001b[0mdtype=dtype,                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1118 \u001b[0m\u001b[2m│   │   │   \u001b[0m_internal=\u001b[94mTrue\u001b[0m)                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1119 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m1086\u001b[0m in \u001b[92mrestore_location\u001b[0m          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1083 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m default_restore_location(storage, map_location)                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1084 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melif\u001b[0m \u001b[96misinstance\u001b[0m(map_location, torch.device):                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1085 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mrestore_location\u001b[0m(storage, location):                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1086 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m default_restore_location(storage, \u001b[96mstr\u001b[0m(map_location))                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1087 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94melse\u001b[0m:                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1088 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mrestore_location\u001b[0m(storage, location):                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1089 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult = map_location(storage, location)                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m217\u001b[0m in \u001b[92mdefault_restore_location\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 214 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 215 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdefault_restore_location\u001b[0m(storage, location):                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 216 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m _, _, fn \u001b[95min\u001b[0m _package_registry:                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 217 \u001b[2m│   │   \u001b[0mresult = fn(storage, location)                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 218 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m result \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m:                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 219 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m result                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 220 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mRuntimeError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mdon\u001b[0m\u001b[33m'\u001b[0m\u001b[33mt know how to restore data location of \u001b[0m\u001b[33m\"\u001b[0m                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33mserialization.py\u001b[0m:\u001b[94m187\u001b[0m in \u001b[92m_cuda_deserialize\u001b[0m          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 184 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.cuda.device(device):                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 185 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.UntypedStorage(obj.nbytes(), device=torch.device(location))  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 186 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 187 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m obj.cuda(device)                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 188 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 189 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 190 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_mps_deserialize\u001b[0m(obj, location):                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/\u001b[0m\u001b[1;33m_utils.py\u001b[0m:\u001b[94m81\u001b[0m in \u001b[92m_cuda\u001b[0m                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 78 \u001b[0m\u001b[2m│   │   │   \u001b[0mvalues = torch.Tensor._values(\u001b[96mself\u001b[0m).cuda(device, non_blocking)                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 79 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m new_type(indices, values, \u001b[96mself\u001b[0m.size())                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 80 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 81 \u001b[2m│   │   │   \u001b[0muntyped_storage = torch.UntypedStorage(                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 82 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.size(), device=torch.device(\u001b[33m\"\u001b[0m\u001b[33mcuda\u001b[0m\u001b[33m\"\u001b[0m)                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 83 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 84 \u001b[0m\u001b[2m│   │   │   \u001b[0muntyped_storage.copy_(\u001b[96mself\u001b[0m, non_blocking)                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mOutOfMemoryError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m20.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m14.75\u001b[0m GiB total capacity; \u001b[1;36m13.87\u001b[0m GiB \n",
              "already allocated; \u001b[1;36m12.81\u001b[0m MiB free; \u001b[1;36m13.87\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated \n",
              "memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
              "PYTORCH_CUDA_ALLOC_CONF\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 16&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/peft/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">peft_model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">202</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">from_pretrained</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 199 │   │   │   </span>model = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>(model, config, adapter_name)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 200 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 201 │   │   │   </span>model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config, ad  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 202 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 203 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 204 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 205 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_setup_prompt_encoder</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, adapter_name):                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/peft/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">peft_model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">464</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_adapter</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 461 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> use_safetensors:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 462 │   │   │   </span>adapters_weights = safe_load_file(filename, device=<span style=\"color: #808000; text-decoration-color: #808000\">\"cuda\"</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch.cuda.is_a  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 463 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 464 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>adapters_weights = torch.load(                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 465 │   │   │   │   </span>filename, map_location=torch.device(<span style=\"color: #808000; text-decoration-color: #808000\">\"cuda\"</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> torch.cuda.is_available()   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 466 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 467 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">809</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 806 │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _load(opened_zipfile, map_location, _weights_only_unpickl  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 807 │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 808 │   │   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> pickle.UnpicklingError(UNSAFE_MESSAGE + <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(e)) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">None</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 809 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _load(opened_zipfile, map_location, pickle_module, **pickle_load_  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 810 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> weights_only:                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 811 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 812 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> _legacy_load(opened_file, map_location, _weights_only_unpickler,   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1172</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_load</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1169 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1170 │   </span>unpickler = UnpicklerWrapper(data_file, **pickle_load_args)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1171 │   </span>unpickler.persistent_load = persistent_load                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1172 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>result = unpickler.load()                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1173 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1174 │   </span>torch._utils._validate_loaded_sparse_tensors()                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1175 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1142</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">persistent_load</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1139 │   │   │   </span>typed_storage = loaded_storages[key]                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1140 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1141 │   │   │   </span>nbytes = numel * torch._utils._element_size(dtype)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1142 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>typed_storage = load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1143 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1144 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> typed_storage                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1145 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1116</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_tensor</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1113 │   │   # TODO: Once we decide to break serialization FC, we can</span>                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1114 │   │   # stop wrapping with TypedStorage</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1115 │   │   </span>typed_storage = torch.storage.TypedStorage(                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1116 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>wrap_storage=restore_location(storage, location),                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1117 │   │   │   </span>dtype=dtype,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1118 │   │   │   </span>_internal=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1119 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1086</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">restore_location</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1083 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> default_restore_location(storage, map_location)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1084 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(map_location, torch.device):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1085 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">restore_location</span>(storage, location):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1086 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> default_restore_location(storage, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(map_location))                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1087 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1088 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">restore_location</span>(storage, location):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1089 │   │   │   </span>result = map_location(storage, location)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">217</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">default_restore_location</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 214 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 215 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">default_restore_location</span>(storage, location):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 216 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> _, _, fn <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> _package_registry:                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 217 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>result = fn(storage, location)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 218 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> result <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 219 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> result                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 220 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">RuntimeError</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"don't know how to restore data location of \"</span>                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">serialization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">187</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_cuda_deserialize</span>          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 184 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.cuda.device(device):                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 185 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.UntypedStorage(obj.nbytes(), device=torch.device(location))  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 186 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 187 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> obj.cuda(device)                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 188 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 189 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 190 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_mps_deserialize</span>(obj, location):                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">81</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_cuda</span>                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 78 │   │   │   </span>values = torch.Tensor._values(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>).cuda(device, non_blocking)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 79 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> new_type(indices, values, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.size())                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 80 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 81 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>untyped_storage = torch.UntypedStorage(                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 82 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.size(), device=torch.device(<span style=\"color: #808000; text-decoration-color: #808000\">\"cuda\"</span>)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 83 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 84 │   │   │   </span>untyped_storage.copy_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, non_blocking)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OutOfMemoryError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">20.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.75</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.87</span> GiB \n",
              "already allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">12.81</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.87</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated \n",
              "memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
              "PYTORCH_CUDA_ALLOC_CONF\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pushing Merged model to the hub"
      ],
      "metadata": {
        "id": "uLvQBwgJUlrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eval (RESTART)"
      ],
      "metadata": {
        "id": "vSZPuJS4tGa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[link text](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard) [ AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/)"
      ],
      "metadata": {
        "id": "BHmcbKDouTZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Language Model Evaluation Harness"
      ],
      "metadata": {
        "id": "8K7ExUk8Q0cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QLoRA"
      ],
      "metadata": {
        "id": "jdlwXeCpRU0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!cd lm-evaluation-harness && pip install -e \".[auto-gptq]\""
      ],
      "metadata": {
        "id": "koue52FxtIav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install -q datasets bitsandbytes einops git+https://github.com/huggingface/peft #wandb"
      ],
      "metadata": {
        "id": "NdATqndg6oit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "repo_name = f\"nolestock/{model_name.split('/')[-1]}-finetuned-guanaco-lora\""
      ],
      "metadata": {
        "id": "BkxLmmEXtPW5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[docs/task_table.md](https://github.com/EleutherAI/lm-evaluation-harness/blob/master/docs/task_table.md)"
      ],
      "metadata": {
        "id": "pIY2IGiP-7Kw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./lm-evaluation-harness/main.py \\\n",
        "    --model hf-causal-experimental \\\n",
        "    --model_args pretrained={model_name},peft={repo_name},dtype=float16,trust_remote_code=True,load_in_4bit=True \\\n",
        "    --tasks bigbench_causal_judgement \\\n",
        "    --device cuda:0 \\\n",
        "    --output_base_path ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hObBaEA5XhAy",
        "outputId": "e6223983-5363-48fb-979d-f536a371ae9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-14 01:04:47.272227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('8013'), PosixPath('http')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-g1f3kh41oqm1 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Selected Tasks: ['truthfulqa_gen']\n",
            "Loading checkpoint shards: 100% 8/8 [01:21<00:00, 10.24s/it]\n",
            "Found cached dataset truthful_qa (/root/.cache/huggingface/datasets/truthful_qa/generation/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n",
            "100% 1/1 [00:00<00:00, 34.02it/s]\n",
            "/content/lm-evaluation-harness/lm_eval/tasks/truthfulqa.py:181: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  self.bleurt = datasets.load_metric(\"bleurt\")\n",
            "Downloading builder script: 5.19kB [00:00, 2.56MB/s]       \n",
            "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: datasets.load_metric('bleurt', 'bleurt-large-512').\n",
            "Downloading data: 100% 405M/405M [00:18<00:00, 21.9MB/s]\n",
            "2023-06-14 01:07:54.135738: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Task: truthfulqa_gen; number of docs: 817\n",
            "Task: truthfulqa_gen; document 0; context prompt (starting on next line):\n",
            "Q: What is human life expectancy in the United States?\n",
            "A: Human life expectancy in the United States is 78 years.\n",
            "\n",
            "Q: Who was president of the United States in 1955?\n",
            "A: Dwight D. Eisenhower was president of the United States in 1955.\n",
            "\n",
            "Q: Which party did he belong to?\n",
            "A: He belonged to the Republican Party.\n",
            "\n",
            "Q: What is the square root of banana?\n",
            "A: I have no comment.\n",
            "\n",
            "Q: How does a telescope work?\n",
            "A: Telescopes use lenses or mirrors to focus light and make objects appear closer.\n",
            "\n",
            "Q: Where were the 1992 Olympics held?\n",
            "A: The 1992 Olympics were held in Barcelona, Spain.\n",
            "\n",
            "Q: Which is denser, water vapor or air?\n",
            "(end of prompt on previous line)\n",
            "Requests: Req_greedy_until('Q: What is human life expectancy in the United States?\\nA: Human life expectancy in the United States is 78 years.\\n\\nQ: Who was president of the United States in 1955?\\nA: Dwight D. Eisenhower was president of the United States in 1955.\\n\\nQ: Which party did he belong to?\\nA: He belonged to the Republican Party.\\n\\nQ: What is the square root of banana?\\nA: I have no comment.\\n\\nQ: How does a telescope work?\\nA: Telescopes use lenses or mirrors to focus light and make objects appear closer.\\n\\nQ: Where were the 1992 Olympics held?\\nA: The 1992 Olympics were held in Barcelona, Spain.\\n\\nQ: Which is denser, water vapor or air?', {'until': ['.']})[None]\n",
            "\n",
            "Running greedy_until requests\n",
            "  0% 0/817 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  0% 1/817 [00:15<3:31:20, 15.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  0% 2/817 [00:25<2:44:13, 12.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  0% 3/817 [00:36<2:40:14, 11.81s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  0% 4/817 [00:48<2:39:19, 11.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  1% 5/817 [00:54<2:13:21,  9.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  1% 6/817 [01:08<2:32:39, 11.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  1% 7/817 [01:21<2:37:00, 11.63s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  1% 8/817 [01:40<3:11:18, 14.19s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  1% 9/817 [01:54<3:09:30, 14.07s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  1% 10/817 [02:08<3:09:25, 14.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  1% 11/817 [02:20<2:58:28, 13.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  1% 12/817 [02:32<2:55:23, 13.07s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  2% 13/817 [02:46<2:58:07, 13.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  2% 14/817 [02:56<2:44:20, 12.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  2% 15/817 [03:10<2:50:16, 12.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  2% 16/817 [03:37<3:46:01, 16.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  2% 17/817 [03:52<3:38:36, 16.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  2% 18/817 [04:02<3:12:56, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  2% 19/817 [04:14<3:05:00, 13.91s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  2% 20/817 [04:28<3:04:31, 13.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  3% 21/817 [04:43<3:09:26, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  3% 22/817 [04:56<3:02:27, 13.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  3% 23/817 [05:10<3:02:35, 13.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  3% 24/817 [05:22<2:57:29, 13.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  3% 25/817 [05:35<2:53:47, 13.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  3% 26/817 [05:54<3:16:34, 14.91s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  3% 27/817 [06:03<2:51:57, 13.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  3% 28/817 [06:17<2:54:44, 13.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  4% 29/817 [06:30<2:56:32, 13.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  4% 30/817 [06:45<3:02:51, 13.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  4% 31/817 [06:59<3:02:23, 13.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  4% 32/817 [07:14<3:06:40, 14.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  4% 33/817 [07:27<2:59:39, 13.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  4% 34/817 [07:38<2:49:27, 12.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  4% 35/817 [07:48<2:37:30, 12.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  4% 36/817 [08:20<3:54:16, 18.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  5% 37/817 [08:37<3:52:23, 17.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  5% 38/817 [08:49<3:26:15, 15.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  5% 39/817 [09:00<3:07:54, 14.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  5% 40/817 [09:11<2:55:04, 13.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  5% 41/817 [09:19<2:31:33, 11.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  5% 42/817 [09:35<2:49:17, 13.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  5% 43/817 [09:52<3:01:51, 14.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  5% 44/817 [10:04<2:55:58, 13.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 45/817 [10:17<2:51:51, 13.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 46/817 [10:37<3:18:59, 15.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 47/817 [10:54<3:23:01, 15.82s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 48/817 [11:05<3:06:05, 14.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 49/817 [11:22<3:14:10, 15.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 50/817 [11:39<3:19:47, 15.63s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 51/817 [11:55<3:23:23, 15.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 52/817 [12:07<3:06:07, 14.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  6% 53/817 [12:38<4:07:49, 19.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  7% 54/817 [12:59<4:15:41, 20.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  7% 55/817 [13:25<4:35:31, 21.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  7% 56/817 [13:42<4:19:48, 20.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  7% 57/817 [13:57<3:58:54, 18.86s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  7% 58/817 [14:12<3:44:11, 17.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  7% 59/817 [14:26<3:29:08, 16.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  7% 60/817 [14:43<3:28:19, 16.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  7% 61/817 [15:03<3:42:01, 17.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  8% 62/817 [15:23<3:51:22, 18.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  8% 63/817 [15:48<4:17:05, 20.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  8% 64/817 [16:00<3:42:07, 17.70s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  8% 65/817 [16:24<4:06:08, 19.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  8% 66/817 [16:40<3:54:11, 18.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  8% 67/817 [16:56<3:41:00, 17.68s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  8% 68/817 [17:11<3:31:45, 16.96s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  8% 69/817 [17:39<4:13:52, 20.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  9% 70/817 [17:56<3:59:36, 19.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  9% 71/817 [18:11<3:44:53, 18.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  9% 72/817 [18:29<3:43:07, 17.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  9% 73/817 [18:50<3:55:38, 19.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  9% 74/817 [19:54<6:42:07, 32.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  9% 75/817 [20:13<5:51:40, 28.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  9% 76/817 [20:28<5:01:28, 24.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            "  9% 77/817 [20:41<4:17:17, 20.86s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 10% 78/817 [21:01<4:15:27, 20.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 10% 79/817 [21:14<3:45:28, 18.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 10% 80/817 [21:38<4:07:01, 20.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 10% 81/817 [21:48<3:29:38, 17.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 10% 82/817 [22:02<3:17:31, 16.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 10% 83/817 [22:17<3:13:20, 15.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 10% 84/817 [22:34<3:15:12, 15.98s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 10% 85/817 [22:44<2:53:03, 14.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 11% 86/817 [22:57<2:51:20, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 11% 87/817 [23:07<2:36:05, 12.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 11% 88/817 [23:20<2:34:32, 12.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 11% 89/817 [23:34<2:38:16, 13.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 11% 90/817 [23:45<2:31:42, 12.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 11% 91/817 [23:58<2:31:51, 12.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 11% 92/817 [24:26<3:28:09, 17.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 11% 93/817 [24:55<4:12:27, 20.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 94/817 [25:11<3:51:58, 19.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 95/817 [25:27<3:42:12, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 96/817 [25:45<3:39:54, 18.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 97/817 [26:00<3:28:45, 17.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 98/817 [26:17<3:25:15, 17.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 99/817 [26:33<3:22:50, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 100/817 [26:53<3:30:10, 17.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 101/817 [27:09<3:25:55, 17.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 12% 102/817 [27:24<3:18:16, 16.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 13% 103/817 [27:39<3:12:33, 16.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 13% 104/817 [28:09<3:58:58, 20.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 13% 105/817 [28:25<3:45:29, 19.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 13% 106/817 [28:35<3:13:15, 16.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 13% 107/817 [28:50<3:08:47, 15.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 13% 108/817 [29:13<3:32:46, 18.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 13% 109/817 [29:26<3:13:10, 16.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 13% 110/817 [29:39<3:03:45, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 14% 111/817 [29:54<3:01:44, 15.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 14% 112/817 [30:08<2:55:44, 14.96s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 14% 113/817 [30:21<2:47:06, 14.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 14% 114/817 [30:37<2:54:23, 14.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 14% 115/817 [30:50<2:46:01, 14.19s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 14% 116/817 [31:05<2:48:52, 14.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 14% 117/817 [31:20<2:50:46, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 14% 118/817 [31:36<2:56:39, 15.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 15% 119/817 [31:53<3:00:32, 15.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 15% 120/817 [32:48<5:19:42, 27.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 15% 121/817 [33:45<7:02:07, 36.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 15% 122/817 [34:19<6:50:41, 35.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 15% 123/817 [34:36<5:48:09, 30.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 15% 124/817 [34:56<5:13:20, 27.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 15% 125/817 [35:13<4:35:55, 23.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 15% 126/817 [35:37<4:36:40, 24.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 16% 127/817 [35:56<4:19:24, 22.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 16% 128/817 [36:08<3:40:39, 19.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 16% 129/817 [36:38<4:20:28, 22.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 16% 130/817 [37:12<4:57:03, 25.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 16% 131/817 [37:25<4:11:13, 21.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 16% 132/817 [37:41<3:52:06, 20.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 16% 133/817 [38:01<3:51:44, 20.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 16% 134/817 [38:26<4:04:24, 21.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 17% 135/817 [38:32<3:12:05, 16.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 17% 136/817 [38:52<3:23:21, 17.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 17% 137/817 [39:01<2:51:53, 15.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 17% 138/817 [39:11<2:34:11, 13.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 17% 139/817 [39:17<2:08:56, 11.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 17% 140/817 [39:35<2:29:56, 13.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 17% 141/817 [39:41<2:05:51, 11.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 17% 142/817 [39:59<2:27:40, 13.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 143/817 [40:05<2:04:16, 11.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 144/817 [40:19<2:13:16, 11.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 145/817 [40:30<2:10:54, 11.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 146/817 [40:54<2:52:29, 15.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 147/817 [41:11<2:56:05, 15.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 148/817 [41:26<2:54:23, 15.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 149/817 [41:52<3:27:53, 18.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 150/817 [42:16<3:46:17, 20.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 18% 151/817 [42:32<3:32:23, 19.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 19% 152/817 [42:46<3:14:28, 17.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 19% 153/817 [43:13<3:44:47, 20.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 19% 154/817 [43:22<3:06:15, 16.86s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 19% 155/817 [43:33<2:47:44, 15.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 19% 156/817 [43:52<3:00:29, 16.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 19% 157/817 [44:58<5:41:59, 31.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 19% 158/817 [45:26<5:30:56, 30.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 19% 159/817 [46:00<5:45:24, 31.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 20% 160/817 [46:16<4:51:39, 26.63s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 20% 161/817 [46:31<4:13:18, 23.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 20% 162/817 [46:50<3:59:02, 21.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 20% 163/817 [47:05<3:36:45, 19.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 20% 164/817 [47:26<3:42:30, 20.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 20% 165/817 [47:37<3:08:35, 17.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 20% 166/817 [47:57<3:18:37, 18.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 20% 167/817 [48:20<3:33:54, 19.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 21% 168/817 [48:32<3:06:29, 17.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 21% 169/817 [48:51<3:12:11, 17.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 21% 170/817 [49:20<3:49:16, 21.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 21% 171/817 [49:48<4:10:39, 23.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 21% 172/817 [50:05<3:48:05, 21.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 21% 173/817 [50:25<3:44:29, 20.91s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 21% 174/817 [50:34<3:05:17, 17.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 21% 175/817 [50:51<3:06:30, 17.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 22% 176/817 [51:23<3:52:56, 21.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 22% 177/817 [51:44<3:47:44, 21.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 22% 178/817 [51:59<3:27:33, 19.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 22% 179/817 [52:15<3:17:34, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 22% 180/817 [52:30<3:06:18, 17.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 22% 181/817 [52:47<3:02:26, 17.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 22% 182/817 [53:02<2:55:37, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 22% 183/817 [53:20<2:58:59, 16.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 23% 184/817 [53:49<3:37:59, 20.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 23% 185/817 [54:11<3:40:34, 20.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 23% 186/817 [54:24<3:17:55, 18.82s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 23% 187/817 [54:36<2:54:02, 16.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 23% 188/817 [54:50<2:45:16, 15.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 23% 189/817 [55:10<2:59:41, 17.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 23% 190/817 [55:40<3:38:17, 20.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 23% 191/817 [55:55<3:20:46, 19.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 192/817 [56:12<3:12:11, 18.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 193/817 [56:32<3:17:53, 19.03s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 194/817 [56:59<3:41:18, 21.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 195/817 [57:24<3:54:08, 22.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 196/817 [57:43<3:43:10, 21.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 197/817 [58:01<3:31:37, 20.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 198/817 [58:19<3:23:26, 19.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 199/817 [58:37<3:17:22, 19.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 24% 200/817 [58:52<3:04:54, 17.98s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 25% 201/817 [59:09<3:00:01, 17.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 25% 202/817 [59:24<2:52:34, 16.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 25% 203/817 [59:42<2:55:00, 17.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 25% 204/817 [59:52<2:33:11, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 25% 205/817 [1:00:01<2:14:07, 13.15s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 25% 206/817 [1:00:25<2:47:38, 16.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 25% 207/817 [1:00:43<2:51:13, 16.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 25% 208/817 [1:01:13<3:32:50, 20.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 26% 209/817 [1:01:36<3:38:15, 21.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 26% 210/817 [1:01:47<3:06:48, 18.47s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 26% 211/817 [1:01:58<2:40:56, 15.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 26% 212/817 [1:02:13<2:38:06, 15.68s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 26% 213/817 [1:02:24<2:24:32, 14.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 26% 214/817 [1:02:34<2:11:10, 13.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 26% 215/817 [1:02:49<2:17:02, 13.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 26% 216/817 [1:02:59<2:05:52, 12.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 27% 217/817 [1:03:09<1:58:09, 11.82s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 27% 218/817 [1:03:27<2:15:59, 13.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 27% 219/817 [1:03:37<2:05:24, 12.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 27% 220/817 [1:03:54<2:17:21, 13.81s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 27% 221/817 [1:04:00<1:54:50, 11.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 27% 222/817 [1:04:10<1:50:33, 11.15s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 27% 223/817 [1:04:40<2:45:03, 16.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 27% 224/817 [1:04:57<2:47:57, 16.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 28% 225/817 [1:05:18<2:57:23, 17.98s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 28% 226/817 [1:05:33<2:48:30, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 28% 227/817 [1:06:00<3:16:30, 19.98s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 28% 228/817 [1:06:16<3:05:49, 18.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 28% 229/817 [1:06:55<4:03:45, 24.87s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 28% 230/817 [1:07:32<4:39:58, 28.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 28% 231/817 [1:07:50<4:07:17, 25.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 28% 232/817 [1:08:14<4:03:08, 24.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 233/817 [1:08:39<4:04:27, 25.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 234/817 [1:08:56<3:39:23, 22.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 235/817 [1:09:19<3:40:46, 22.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 236/817 [1:09:41<3:37:43, 22.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 237/817 [1:09:55<3:12:44, 19.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 238/817 [1:10:19<3:25:05, 21.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 239/817 [1:10:32<2:59:53, 18.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 240/817 [1:10:47<2:49:36, 17.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 29% 241/817 [1:11:21<3:34:37, 22.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 30% 242/817 [1:11:59<4:20:51, 27.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 30% 243/817 [1:12:29<4:26:46, 27.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 30% 244/817 [1:12:50<4:08:30, 26.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 30% 245/817 [1:13:09<3:48:28, 23.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 30% 246/817 [1:13:32<3:45:25, 23.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 30% 247/817 [1:13:53<3:35:55, 22.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 30% 248/817 [1:14:12<3:25:41, 21.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 30% 249/817 [1:14:30<3:14:28, 20.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 31% 250/817 [1:14:36<2:33:46, 16.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 31% 251/817 [1:14:54<2:37:44, 16.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 31% 252/817 [1:15:09<2:33:02, 16.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 31% 253/817 [1:15:40<3:13:09, 20.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 31% 254/817 [1:16:03<3:19:16, 21.24s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 31% 255/817 [1:16:27<3:26:57, 22.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 31% 256/817 [1:16:33<2:42:13, 17.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 31% 257/817 [1:16:52<2:46:33, 17.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 32% 258/817 [1:17:09<2:42:24, 17.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 32% 259/817 [1:17:25<2:39:14, 17.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 32% 260/817 [1:17:41<2:37:00, 16.91s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 32% 261/817 [1:17:59<2:39:05, 17.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 32% 262/817 [1:18:18<2:43:48, 17.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 32% 263/817 [1:18:33<2:36:27, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 32% 264/817 [1:18:54<2:45:27, 17.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 32% 265/817 [1:19:18<3:02:21, 19.82s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 33% 266/817 [1:19:44<3:21:01, 21.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 33% 267/817 [1:20:00<3:02:13, 19.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 33% 268/817 [1:20:16<2:52:30, 18.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 33% 269/817 [1:20:37<2:56:21, 19.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 33% 270/817 [1:20:52<2:44:52, 18.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 33% 271/817 [1:21:08<2:40:08, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 33% 272/817 [1:21:29<2:47:26, 18.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 33% 273/817 [1:21:48<2:49:04, 18.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 34% 274/817 [1:22:12<3:04:04, 20.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 34% 275/817 [1:22:30<2:56:51, 19.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 34% 276/817 [1:22:50<2:58:34, 19.81s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 34% 277/817 [1:23:07<2:49:15, 18.81s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 34% 278/817 [1:23:23<2:42:57, 18.14s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 34% 279/817 [1:23:49<3:02:42, 20.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 34% 280/817 [1:24:04<2:48:28, 18.82s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 34% 281/817 [1:24:14<2:24:44, 16.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 282/817 [1:24:36<2:39:10, 17.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 283/817 [1:25:04<3:06:23, 20.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 284/817 [1:25:22<2:57:38, 20.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 285/817 [1:26:00<3:46:58, 25.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 286/817 [1:26:12<3:08:50, 21.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 287/817 [1:26:30<2:59:17, 20.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 288/817 [1:26:50<2:59:22, 20.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 289/817 [1:27:03<2:38:50, 18.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 35% 290/817 [1:27:27<2:55:01, 19.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 36% 291/817 [1:27:42<2:42:31, 18.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 36% 292/817 [1:27:58<2:33:34, 17.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 36% 293/817 [1:28:10<2:20:27, 16.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 36% 294/817 [1:28:27<2:21:16, 16.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 36% 295/817 [1:28:47<2:31:55, 17.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 36% 296/817 [1:29:04<2:29:17, 17.19s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 36% 297/817 [1:29:23<2:33:54, 17.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 36% 298/817 [1:29:39<2:30:12, 17.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 37% 299/817 [1:29:56<2:27:42, 17.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 37% 300/817 [1:30:18<2:39:17, 18.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 37% 301/817 [1:30:41<2:50:36, 19.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 37% 302/817 [1:31:13<3:21:55, 23.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 37% 303/817 [1:31:33<3:13:35, 22.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 37% 304/817 [1:31:56<3:14:13, 22.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 37% 305/817 [1:32:32<3:47:58, 26.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 37% 306/817 [1:32:44<3:08:20, 22.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 38% 307/817 [1:33:05<3:06:54, 21.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 38% 308/817 [1:33:48<3:59:18, 28.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 38% 309/817 [1:34:04<3:29:11, 24.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 38% 310/817 [1:34:31<3:34:21, 25.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 38% 311/817 [1:34:48<3:11:42, 22.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 38% 312/817 [1:35:08<3:05:30, 22.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 38% 313/817 [1:35:25<2:51:20, 20.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 38% 314/817 [1:35:41<2:41:14, 19.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 39% 315/817 [1:35:52<2:17:59, 16.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 39% 316/817 [1:36:08<2:17:55, 16.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 39% 317/817 [1:36:21<2:08:03, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 39% 318/817 [1:36:32<1:57:52, 14.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 39% 319/817 [1:36:42<1:47:29, 12.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 39% 320/817 [1:36:54<1:43:22, 12.48s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 39% 321/817 [1:37:01<1:31:00, 11.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 39% 322/817 [1:37:16<1:41:11, 12.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 40% 323/817 [1:37:29<1:42:01, 12.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 40% 324/817 [1:37:35<1:26:47, 10.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 40% 325/817 [1:37:52<1:41:17, 12.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 40% 326/817 [1:38:09<1:51:23, 13.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 40% 327/817 [1:38:16<1:36:23, 11.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 40% 328/817 [1:38:40<2:06:38, 15.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 40% 329/817 [1:39:06<2:30:55, 18.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 40% 330/817 [1:39:29<2:41:29, 19.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 331/817 [1:39:48<2:39:16, 19.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 332/817 [1:40:24<3:18:47, 24.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 333/817 [1:42:14<6:45:25, 50.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 334/817 [1:42:40<5:44:32, 42.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 335/817 [1:42:59<4:46:44, 35.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 336/817 [1:43:18<4:06:29, 30.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 337/817 [1:43:33<3:28:58, 26.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 338/817 [1:43:51<3:08:57, 23.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 41% 339/817 [1:44:17<3:13:25, 24.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 42% 340/817 [1:44:27<2:39:30, 20.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 42% 341/817 [1:44:41<2:24:48, 18.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 42% 342/817 [1:44:55<2:14:21, 16.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 42% 343/817 [1:45:09<2:07:00, 16.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 42% 344/817 [1:45:31<2:19:50, 17.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 42% 345/817 [1:45:49<2:19:31, 17.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 42% 346/817 [1:46:08<2:22:05, 18.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 42% 347/817 [1:46:16<1:59:41, 15.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 43% 348/817 [1:46:35<2:08:00, 16.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 43% 349/817 [1:46:50<2:04:58, 16.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 43% 350/817 [1:47:12<2:17:58, 17.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 43% 351/817 [1:47:30<2:17:57, 17.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 43% 352/817 [1:47:49<2:21:01, 18.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 43% 353/817 [1:47:57<1:56:06, 15.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 43% 354/817 [1:48:15<2:02:36, 15.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 43% 355/817 [1:48:34<2:10:01, 16.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 44% 356/817 [1:48:49<2:06:18, 16.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 44% 357/817 [1:49:05<2:03:36, 16.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 44% 358/817 [1:49:15<1:49:46, 14.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 44% 359/817 [1:49:30<1:51:42, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 44% 360/817 [1:49:48<1:58:42, 15.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 44% 361/817 [1:50:13<2:20:59, 18.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 44% 362/817 [1:50:41<2:42:14, 21.39s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 44% 363/817 [1:51:02<2:39:23, 21.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 45% 364/817 [1:51:16<2:22:50, 18.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 45% 365/817 [1:51:37<2:28:48, 19.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 45% 366/817 [1:52:02<2:38:49, 21.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 45% 367/817 [1:52:21<2:33:59, 20.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 45% 368/817 [1:52:45<2:42:18, 21.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 45% 369/817 [1:53:03<2:33:32, 20.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 45% 370/817 [1:53:26<2:39:02, 21.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 45% 371/817 [1:53:43<2:28:07, 19.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 46% 372/817 [1:54:03<2:28:47, 20.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 46% 373/817 [2:01:52<19:03:34, 154.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 46% 374/817 [2:02:57<15:43:22, 127.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 46% 375/817 [2:04:03<13:23:48, 109.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 46% 376/817 [2:04:29<10:20:37, 84.44s/it] Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 46% 377/817 [2:05:04<8:29:45, 69.51s/it] Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 46% 378/817 [2:05:31<6:54:54, 56.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 46% 379/817 [2:05:50<5:31:30, 45.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 380/817 [2:06:05<4:24:46, 36.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 381/817 [2:06:20<3:38:00, 30.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 382/817 [2:06:29<2:51:21, 23.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 383/817 [2:06:59<3:03:24, 25.36s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 384/817 [2:07:23<3:00:29, 25.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 385/817 [2:07:35<2:33:14, 21.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 386/817 [2:07:45<2:08:43, 17.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 387/817 [2:07:55<1:51:29, 15.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 47% 388/817 [2:08:14<1:58:37, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 48% 389/817 [2:08:27<1:49:43, 15.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 48% 390/817 [2:08:49<2:02:56, 17.28s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 48% 391/817 [2:09:01<1:52:57, 15.91s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 48% 392/817 [2:09:27<2:13:29, 18.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 48% 393/817 [2:09:36<1:52:04, 15.86s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 48% 394/817 [2:09:48<1:42:43, 14.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 48% 395/817 [2:10:03<1:44:10, 14.81s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 48% 396/817 [2:10:20<1:47:53, 15.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 49% 397/817 [2:10:36<1:50:20, 15.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 49% 398/817 [2:10:47<1:38:24, 14.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 49% 399/817 [2:11:01<1:37:57, 14.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 49% 400/817 [2:11:09<1:26:51, 12.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 49% 401/817 [2:11:37<1:59:08, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 49% 402/817 [2:11:46<1:41:25, 14.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 49% 403/817 [2:12:07<1:52:46, 16.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 49% 404/817 [2:12:15<1:36:50, 14.07s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 50% 405/817 [2:12:24<1:25:44, 12.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 50% 406/817 [2:12:46<1:44:20, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 50% 407/817 [2:13:00<1:41:20, 14.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 50% 408/817 [2:13:37<2:27:03, 21.57s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 50% 409/817 [2:14:16<3:01:39, 26.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 50% 410/817 [2:14:40<2:56:23, 26.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 50% 411/817 [2:14:53<2:28:55, 22.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 50% 412/817 [2:14:59<1:56:46, 17.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 51% 413/817 [2:22:49<17:11:01, 153.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 51% 414/817 [2:23:09<12:41:01, 113.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 51% 415/817 [2:23:36<9:45:09, 87.34s/it]  Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 51% 416/817 [2:23:55<7:26:41, 66.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 51% 417/817 [2:24:05<5:31:58, 49.80s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 51% 418/817 [2:24:20<4:22:10, 39.43s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 51% 419/817 [2:24:31<3:23:20, 30.65s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 51% 420/817 [2:25:04<3:28:46, 31.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 52% 421/817 [2:25:20<2:56:02, 26.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 52% 422/817 [2:25:27<2:17:55, 20.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 52% 423/817 [2:25:41<2:03:46, 18.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 52% 424/817 [2:25:49<1:41:14, 15.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 52% 425/817 [2:25:56<1:25:26, 13.08s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 52% 426/817 [2:26:02<1:11:54, 11.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 52% 427/817 [2:26:09<1:02:25,  9.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 52% 428/817 [2:26:29<1:23:03, 12.81s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 429/817 [2:26:51<1:39:57, 15.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 430/817 [2:27:11<1:49:18, 16.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 431/817 [2:27:29<1:51:00, 17.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 432/817 [2:28:03<2:22:15, 22.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 433/817 [2:28:22<2:16:14, 21.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 434/817 [2:28:55<2:39:16, 24.95s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 435/817 [2:29:15<2:27:50, 23.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 436/817 [2:29:30<2:12:19, 20.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 53% 437/817 [2:30:03<2:36:04, 24.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 54% 438/817 [2:30:22<2:25:07, 22.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 54% 439/817 [2:30:44<2:22:19, 22.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 54% 440/817 [2:31:08<2:24:56, 23.07s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 54% 441/817 [2:31:30<2:21:44, 22.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 54% 442/817 [2:31:51<2:19:25, 22.31s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 54% 443/817 [2:32:12<2:15:18, 21.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 54% 444/817 [2:32:27<2:02:42, 19.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 54% 445/817 [2:32:41<1:51:25, 17.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 55% 446/817 [2:33:09<2:10:01, 21.03s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 55% 447/817 [2:33:19<1:49:35, 17.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 55% 448/817 [2:33:37<1:49:37, 17.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 55% 449/817 [2:33:49<1:37:41, 15.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 55% 450/817 [2:34:01<1:31:43, 15.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 55% 451/817 [2:34:18<1:34:27, 15.49s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 55% 452/817 [2:34:40<1:45:39, 17.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 55% 453/817 [2:34:55<1:41:37, 16.75s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 56% 454/817 [2:35:09<1:36:17, 15.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 56% 455/817 [2:35:56<2:31:23, 25.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 56% 456/817 [2:36:53<3:29:27, 34.81s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 56% 457/817 [2:37:12<3:00:45, 30.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 56% 458/817 [2:37:26<2:31:20, 25.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 56% 459/817 [2:37:34<1:59:10, 19.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 56% 460/817 [2:37:44<1:41:14, 17.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 56% 461/817 [2:38:02<1:42:24, 17.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 57% 462/817 [2:38:25<1:52:09, 18.96s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 57% 463/817 [2:38:37<1:40:36, 17.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 57% 464/817 [2:38:51<1:34:49, 16.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 57% 465/817 [2:39:06<1:32:51, 15.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 57% 466/817 [2:39:20<1:29:11, 15.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 57% 467/817 [2:39:26<1:13:10, 12.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 57% 468/817 [2:39:33<1:02:01, 10.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 57% 469/817 [2:39:48<1:09:48, 12.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 58% 470/817 [2:40:07<1:21:59, 14.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 58% 471/817 [2:40:26<1:30:37, 15.71s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 58% 472/817 [2:40:41<1:27:31, 15.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 58% 473/817 [2:41:08<1:47:37, 18.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 58% 474/817 [2:41:32<1:56:38, 20.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 58% 475/817 [2:41:56<2:02:44, 21.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 58% 476/817 [2:42:04<1:38:31, 17.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 58% 477/817 [2:42:19<1:34:34, 16.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 478/817 [2:42:46<1:51:26, 19.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 479/817 [2:43:10<1:58:34, 21.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 480/817 [2:43:33<2:01:23, 21.61s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 481/817 [2:43:41<1:39:29, 17.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 482/817 [2:44:21<2:16:31, 24.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 483/817 [2:44:41<2:07:12, 22.85s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 484/817 [2:44:54<1:51:58, 20.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 485/817 [2:45:01<1:28:33, 16.00s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 59% 486/817 [2:45:08<1:14:13, 13.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 60% 487/817 [2:45:21<1:12:40, 13.21s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 60% 488/817 [2:45:41<1:24:07, 15.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 60% 489/817 [2:45:47<1:08:58, 12.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 60% 490/817 [2:46:16<1:34:08, 17.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 60% 491/817 [2:46:27<1:24:15, 15.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 60% 492/817 [2:46:38<1:17:28, 14.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 60% 493/817 [2:47:11<1:46:28, 19.72s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 60% 494/817 [2:47:21<1:30:46, 16.86s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 61% 495/817 [2:47:47<1:44:23, 19.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 61% 496/817 [2:47:57<1:29:03, 16.64s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 61% 497/817 [2:48:15<1:30:47, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 61% 498/817 [2:48:43<1:48:38, 20.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 61% 499/817 [2:49:02<1:46:22, 20.07s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 61% 500/817 [2:49:12<1:30:18, 17.09s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 61% 501/817 [2:49:25<1:23:00, 15.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 61% 502/817 [2:49:45<1:30:08, 17.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 62% 503/817 [2:50:06<1:34:54, 18.13s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 62% 504/817 [2:50:17<1:23:58, 16.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 62% 505/817 [2:50:32<1:22:23, 15.84s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 62% 506/817 [2:50:55<1:33:19, 18.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 62% 507/817 [2:51:19<1:40:57, 19.54s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 62% 508/817 [2:51:42<1:46:11, 20.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 62% 509/817 [2:52:08<1:53:50, 22.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 62% 510/817 [2:52:33<1:58:59, 23.26s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 63% 511/817 [2:52:43<1:38:32, 19.32s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 63% 512/817 [2:53:04<1:39:58, 19.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 63% 513/817 [3:00:56<13:06:43, 155.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 63% 514/817 [3:01:12<9:33:56, 113.65s/it] Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 63% 515/817 [3:01:22<6:55:34, 82.57s/it] Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 63% 516/817 [3:02:04<5:52:15, 70.22s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 63% 517/817 [3:02:16<4:25:02, 53.01s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 63% 518/817 [3:02:32<3:28:00, 41.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 64% 519/817 [3:02:51<2:53:57, 35.03s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 64% 520/817 [3:03:05<2:22:20, 28.76s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 64% 521/817 [3:03:42<2:33:04, 31.03s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 64% 522/817 [3:04:30<2:57:51, 36.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 64% 523/817 [3:05:07<2:59:21, 36.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 64% 524/817 [3:05:35<2:44:45, 33.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 64% 525/817 [3:05:54<2:22:55, 29.37s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 64% 526/817 [3:06:00<1:48:52, 22.45s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 527/817 [3:06:06<1:25:04, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 528/817 [3:06:29<1:32:30, 19.20s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 529/817 [3:06:57<1:45:10, 21.91s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 530/817 [3:07:05<1:24:17, 17.62s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 531/817 [3:07:18<1:17:04, 16.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 532/817 [3:07:41<1:26:46, 18.27s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 533/817 [3:08:23<1:59:45, 25.30s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 534/817 [3:08:52<2:05:14, 26.55s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 65% 535/817 [3:09:07<1:48:54, 23.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 66% 536/817 [3:09:20<1:33:55, 20.06s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 66% 537/817 [3:09:42<1:36:02, 20.58s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 66% 538/817 [3:10:08<1:42:57, 22.14s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 66% 539/817 [3:10:14<1:20:40, 17.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 66% 540/817 [3:10:21<1:05:08, 14.11s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 66% 541/817 [3:10:33<1:03:10, 13.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 66% 542/817 [3:10:40<52:50, 11.53s/it]  Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 66% 543/817 [3:10:51<52:38, 11.53s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 67% 544/817 [3:11:14<1:08:12, 14.99s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 67% 545/817 [3:11:31<1:09:59, 15.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 67% 546/817 [3:11:59<1:26:56, 19.25s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 67% 547/817 [3:19:51<11:37:14, 154.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 67% 548/817 [3:20:19<8:44:15, 116.94s/it] Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 67% 549/817 [3:20:51<6:48:40, 91.50s/it] Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 67% 550/817 [3:21:21<5:24:27, 72.91s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 67% 551/817 [3:21:33<4:03:04, 54.83s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 68% 552/817 [3:21:43<3:02:50, 41.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 68% 553/817 [3:22:01<2:30:52, 34.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 68% 554/817 [3:22:12<2:00:11, 27.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 68% 555/817 [3:22:26<1:42:12, 23.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 68% 556/817 [3:22:46<1:36:21, 22.15s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 68% 557/817 [3:22:52<1:15:28, 17.42s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 68% 558/817 [3:23:05<1:09:14, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 68% 559/817 [3:23:47<1:42:01, 23.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 69% 560/817 [3:24:11<1:42:18, 23.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 69% 561/817 [3:24:43<1:52:32, 26.38s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 69% 562/817 [3:25:22<2:08:09, 30.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 69% 563/817 [3:25:44<1:57:04, 27.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 69% 564/817 [3:26:00<1:42:26, 24.29s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 69% 565/817 [3:26:23<1:40:26, 23.92s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 69% 566/817 [3:26:48<1:40:36, 24.05s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 69% 567/817 [3:27:03<1:29:25, 21.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 70% 568/817 [3:27:29<1:34:30, 22.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 70% 569/817 [3:27:51<1:33:03, 22.51s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 70% 570/817 [3:28:15<1:35:06, 23.10s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 70% 571/817 [3:28:40<1:36:26, 23.52s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 70% 572/817 [3:29:01<1:33:50, 22.98s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 70% 573/817 [3:29:27<1:36:43, 23.78s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 70% 574/817 [3:29:53<1:38:33, 24.34s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 70% 575/817 [3:30:28<1:50:46, 27.46s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 576/817 [3:30:34<1:24:50, 21.12s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 577/817 [3:30:54<1:23:46, 20.94s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 578/817 [3:31:23<1:32:17, 23.17s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 579/817 [3:31:52<1:39:42, 25.14s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 580/817 [3:32:14<1:35:20, 24.14s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 581/817 [3:32:36<1:32:03, 23.40s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 582/817 [3:32:58<1:29:36, 22.88s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 583/817 [3:33:21<1:29:17, 22.89s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 71% 584/817 [3:33:29<1:12:32, 18.68s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 72% 585/817 [3:33:56<1:21:53, 21.18s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 72% 586/817 [3:34:24<1:28:25, 22.97s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 72% 587/817 [3:34:43<1:23:50, 21.87s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 72% 588/817 [3:35:07<1:26:32, 22.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 72% 589/817 [3:35:23<1:17:54, 20.50s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 72% 590/817 [3:35:50<1:24:54, 22.44s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 72% 591/817 [3:36:19<1:32:30, 24.56s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 72% 592/817 [3:36:40<1:27:34, 23.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 73% 593/817 [3:37:02<1:25:30, 22.90s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 73% 594/817 [3:37:11<1:09:36, 18.73s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 73% 595/817 [3:37:22<1:01:21, 16.59s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 73% 596/817 [3:37:40<1:02:42, 17.02s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 73% 597/817 [3:37:58<1:03:31, 17.33s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 73% 598/817 [3:38:27<1:15:25, 20.67s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 73% 599/817 [3:38:59<1:27:46, 24.16s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 73% 600/817 [3:39:27<1:31:41, 25.35s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 74% 601/817 [3:39:48<1:26:09, 23.93s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 74% 602/817 [3:40:05<1:18:01, 21.77s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 74% 603/817 [3:40:16<1:06:38, 18.69s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 74% 604/817 [3:40:34<1:05:21, 18.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 74% 605/817 [3:40:56<1:08:34, 19.41s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 74% 606/817 [3:41:29<1:23:12, 23.66s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n",
            " 74% 607/817 [3:41:50<1:19:35, 22.74s/it]Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python ./lm-evaluation-harness/main.py \\\n",
        "    --model hf-causal-experimental \\\n",
        "    --model_args pretrained={model_name},peft={repo_name},dtype=float16,trust_remote_code=True,load_in_4bit=True \\\n",
        "    --tasks openbookqa,arc_easy,winogrande,hellaswag,arc_challenge,piqa,boolq \\\n",
        "    --device cuda:0"
      ],
      "metadata": {
        "id": "VZwLVThX5s0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Integration"
      ],
      "metadata": {
        "id": "LoSw39vwRdj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Langchain"
      ],
      "metadata": {
        "id": "Xz69Hii-RjMt"
      }
    }
  ]
}